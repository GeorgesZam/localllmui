name: Build Local Chat

on:
  push:
    branches: [main, master]
  workflow_dispatch:

jobs:
  # === WINDOWS ===
  build-windows:
    runs-on: windows-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install deps
        run: |
          pip install pyinstaller
          pip install llama-cpp-python --prefer-binary
          pip install sentence-transformers
          pip install numpy
      
      - name: Download model
        run: |
          mkdir models
          curl -L -o models/model.gguf https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q4_k_m.gguf
        shell: bash
      
      - name: Create data folder
        run: mkdir data
      
      - name: Build
        run: |
          $llamaPath = python -c "import llama_cpp; import os; print(os.path.dirname(llama_cpp.__file__))"
          cd src
          pyinstaller --onefile --windowed --name "LocalChat" --add-data "../models;models" --add-data "../data;data" --add-data "$llamaPath;llama_cpp" --collect-all llama_cpp --hidden-import llama_cpp --hidden-import sentence_transformers --collect-all sentence_transformers main.py
        shell: pwsh
      
      - uses: actions/upload-artifact@v4
        with:
          name: LocalChat-Windows
          path: src/dist/*.exe

  # === MACOS ===
  build-macos:
    runs-on: macos-14
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install build tools
        run: brew install cmake
      
      - name: Install deps
        env:
          CMAKE_ARGS: "-DLLAMA_METAL=on"
        run: |
          pip install pyinstaller
          pip install llama-cpp-python --no-cache-dir
          pip install sentence-transformers
          pip install numpy
      
      - name: Download model
        run: |
          mkdir models
          curl -L -o models/model.gguf https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q4_k_m.gguf
      
      - name: Create data folder
        run: mkdir data
      
      - name: Build app
        run: |
          LLAMA_PATH=$(python -c "import llama_cpp; import os; print(os.path.dirname(llama_cpp.__file__))")
          echo "LLAMA_PATH: $LLAMA_PATH"
          ls -la "$LLAMA_PATH"
          ls -la "$LLAMA_PATH/lib" || true
          
          cd src
          pyinstaller --onefile --windowed --name "LocalChat" \
            --add-data "../models:models" \
            --add-data "../data:data" \
            --add-data "$LLAMA_PATH:llama_cpp" \
            --collect-all llama_cpp \
            --collect-binaries llama_cpp \
            --hidden-import llama_cpp \
            --hidden-import sentence_transformers \
            --collect-all sentence_transformers \
            main.py
      
      - name: Create ZIP
        run: |
          cd src/dist
          zip -r ../../LocalChat-macOS.zip LocalChat.app
      
      - uses: actions/upload-artifact@v4
        with:
          name: LocalChat-macOS
          path: LocalChat-macOS.zip
